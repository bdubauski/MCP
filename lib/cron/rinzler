#!/usr/bin/env python3
#
# rinzler
#   check for updates in the repo, then load the build targets and dependancies into the database
#
import os

os.environ.setdefault( 'DJANGO_SETTINGS_MODULE', 'mcp.settings' )

import django
django.setup()

import logging
import sys
import glob
import shutil
import socket
import json
import cinp
import re

from ssl import SSLError
from datetime import datetime, timezone
from logging.handlers import SysLogHandler

from django.db.models import ProtectedError

from mcp.Project.models import Project, Commit, Build, Package, BuildDependancy, BuildResource
from mcp.Resource.models import Resource
from mcp.lib.Makefile import Makefile, MakeException
from mcp.lib.t3kton import getContractor
from mcp.lib.Slack import getSlack
from mcp.lib.Packrat import getPackrat

WORK_DIR = '/tmp/rinzler'
PID_FILE = '/var/run/rinzler.pid'

name_regex = re.compile( '^[a-zA-Z0-9][a-zA-Z0-9_\-]*$' )  # from contractor/fields.py

blueprint_cache_map = {}

# when loading build targets, make sure the target name isn't reserved: 'lint', 'test', 'rpm', 'dpkg', 'respkg', 'resource', 'doc', 'all', 'clean', '*-targets', '*-requires', '*-depends', '*-file', '*-builds', 'target'

logging.basicConfig()
logger = logging.getLogger()
handler = SysLogHandler( address='/dev/log', facility=SysLogHandler.LOG_DAEMON )
handler.setFormatter( logging.Formatter( fmt='recognizer[%(process)d]: %(message)s' ) )
logger.addHandler( handler )
logging.info( 'Starting...' )
if '-v' in sys.argv:
  logger.setLevel( logging.DEBUG )
elif '-c' in sys.argv:
  logger.setLevel( logging.ERROR )
else:
  logger.setLevel( logging.INFO )

if os.path.exists( PID_FILE ):
  logging.error( 'pid file exists, bailing...' )
  sys.exit( 0 )

tmp = open( PID_FILE, 'w' )
tmp.write( '{0}\n'.format( os.getpid() ) )
tmp.close()

slack = getSlack( 'rinzler' )
contractor = getContractor()


def _checkBluePrint( blueprint ):
  global blueprint_cache_map

  if blueprint in blueprint_cache_map:
    return blueprint_cache_map[ blueprint ]

  try:
    contractor.getBluePrint( blueprint )
    blueprint_cache_map[ blueprint ] = True
  except cinp.client.NotFound:
    blueprint_cache_map[ blueprint ] = False

  return blueprint_cache_map[ blueprint ]


def _cleanInterfaceMap( interface_map ):
  if interface_map == {}:
    return {}

  if not isinstance( interface_map, dict ):
    raise Exception( 'interface_map must be a dict' )

  name_list = sorted( interface_map.keys() )  # python dicts to not stay sorted, so we are going to do it alphabetically for now
  has_primary = False

  result = {}

  for name in name_list:
    if not name_regex.match( name ) or len( name ) > 20:  # from contractor/Utilities/models.py NetworkInterface name length
      raise ValueError( 'Interface name "{0}" is invalid'.format( name ) )

    result[ name ] = {
                       'is_primary': False
                     }

    for item in ( 'network', ):
      try:
        result[ name ][ item ] = interface_map[ name ][ item ]
      except KeyError:
        pass

    for item in ( 'offset', ):
      try:
        result[ name ][ item ] = int( interface_map[ name ][ item ] )
      except KeyError:
        pass
      except ValueError:
        raise Exception( 'Invalid value for integer for "{0}"'.format( item ) )

    for item in ( 'is_primary', ):
      try:
        result[ name ][ item ] = bool( interface_map[ name ][ item ] )
      except KeyError:
        pass
      except ValueError:
        raise Exception( 'Invalid value for boolean for "{0}"'.format( item ) )

    has_primary |= result[ name ].get( 'is_primary', False )

  if not has_primary:
    result[ name_list[0] ][ 'is_primary' ] = True

  return result


def _makeBuild( build_name, project, manual ):
  resource_list = make.resources( build_name )
  network_list = make.networks( build_name )
  if manual:
    logging.debug( 'Build: "{0}" resources: "{1}" networks: "{2}".'.format( build_name, resource_list, network_list ) )
  else:
    dependancy_list = make.depends( build_name )
    logging.debug( 'Build: "{0}" depends: "{1}" resources: "{2}" networks: "{3}".'.format( build_name, dependancy_list, resource_list, network_list ) )

  if not resource_list:
    return 'Build "{0}" has no resources'.format( build_name )

  package_map = {}
  if not manual:
    for dependancy in dependancy_list:
      ( package_name, tag ) = dependancy.split( ':' )
      try:
        Package.objects.get( name=package_name )

      except Package.DoesNotExist:
        packrat = getPackrat()
        packrat_package_uri = '/api/v2/Package/Package:{0}:'.format( package_name )
        try:
          packrat.cinp.get( packrat_package_uri )
        except cinp.client.NotFound:
          packrat_package_uri = None

        packrat.logout()

        if packrat_package_uri is None:
          logging.warn( 'Package "{0}" for dependancy for build "{1}" not found in Packrat, skipping dependancy.'.format( package_name, build_name ) )
          continue

        package = Package( name=package_name, packrat_id=packrat_package_uri )
        package.full_clean()
        package.save()

      package_map[ package_name ] = tag

    if not package_map:
      return 'All Packages for Build "{0}" are not found in Packrat, skipping build.'.format( build_name )

  try:
    build = project.build_set.get( name=build_name )
    build.manual = manual
    build.full_clean()
    build.save()

    for item in build.buildresource_set.all():  # TODO: update instead of delete and re-add
      item.delete()

    for item in build.builddependancy_set.all():
      item.delete()

  except Build.DoesNotExist:
    build = Build()
    build.name = build_name
    build.project = project
    build.manual = manual
    build.full_clean()
    build.save()

  build.network_map = {}
  for network in network_list:
    ( name, data ) = network.split( ':', 1 )
    try:
       data = json.loads( data )
    except ( ValueError, json.JSONDecodeError ):
      build.delete()
      return 'Error parsing network info: "{0}", killing the build'.format( data )

    build.network_map[ name ] = {}
    for item, default in ( ( 'dedicated', False ), ):
      build.network_map[ name ][ item ] = bool( data.get( item, default ) )

    for item, default in ( ( 'min_addresses', 14 ), ):  # a /28
      build.network_map[ name ][ item ] = int( data.get( item, default ) )

  build.full_clean()
  build.save()

  for resource in resource_list:
    ( name, data ) = resource.split( ':', 1 )
    try:
       data = json.loads( data )
    except ( ValueError, json.JSONDecodeError ):
      build.delete()
      return 'Error parsing resource info: "{0}", killing the build'.format( data )

    if 'resource' not in data or 'blueprint' not in data:
      return '"resource" and/or "blueprint" is missing from resource "{0}"'.format( name )

    try:
      resource = Resource.objects.get( name=data[ 'resource' ] )
    except Resource.DoesNotExist:
      build.delete()
      return 'Resource "{0}" for build "{1}" does not exist, killing the build.'.format( data[ 'resource' ], build_name )

    if not _checkBluePrint( data[ 'blueprint' ] ):
      return 'Blueprint "{0}" not found'.format( data[ 'blueprint' ] )

    buildres = BuildResource()
    buildres.build = build
    buildres.resource = resource
    buildres.blueprint = data[ 'blueprint' ]
    buildres.name = name
    buildres.quantity = int( data.get( 'quantity', 1 ) )
    buildres.interface_map = _cleanInterfaceMap( data.get( 'interface_map', {} ) )
    buildres.config_values = data.get( 'config_values', {} )
    buildres.autorun = data.get( 'autorun', False )
    buildres.full_clean()
    buildres.save()

  for package_name, tag in package_map.items():
    package = Package.objects.get( name=package_name )

    builddep = BuildDependancy()
    builddep.build = build
    builddep.package = package
    builddep.tag = tag
    builddep.full_clean()
    builddep.save()

  return None


def _failCommit( commit, message ):
  commit.lint_results = { 'makefile': { 'success': False, 'status': 'Failed', 'results': message } }
  commit.done_at = datetime.now( timezone.utc )
  commit.full_clean()
  commit.save()

  commit.postResults()


for project in Project.objects.all().order_by( 'last_checked' ):
  # skip internal projects
  if project.name.startswith( '_' ):
    continue

  # skip any busy projects
  if project.busy:
    continue

  if project.commit_set.all().count() > project.max_commit_count:
    # just delete the last one, this get's called enough it will eventually get the list trimmed down
    project.commit_set.all().order_by( 'created' )[0].delete()

  url = project.clone_git_url

  if url is None:  # for now we only support Git based projects
    continue

  # it's good let's do this
  logging.info( 'Checking project "{0}"'.format( project.name ) )

  if not project.local_path:
    project.local_path = '{0}/{1}'.format( project.pk, os.path.basename( url ) )
    logging.debug( 'Creating "{0}" in "{1}"...'.format( project.name, project.local_path ) )

    git = project.internal_git  # can't get internal_git until after local_path is set
    git.setup( url )  # TODO: make sure we can check things out before saving the path
    project.full_clean()
    project.save()

  else:
    logging.debug( 'Updating "{0}"...'.format( project.name ) )
    git = project.internal_git

  git.update()

  try:
    scm = project.scm
  except ( socket.timeout, socket.error, SSLError ):
    logging.warning( 'Connection Error connecting to remote scm, will try again later.' )
    continue

  try:
    merge_list = scm.getMergeList()
  except ( socket.timeout, socket.error, SSLError ):
    logging.warning( 'Connection Error getting Merge List, will try again later.' )
    continue

  for branch in git.ref_map():
    merge = scm.branchToMerge( branch )
    if merge is not None and merge not in merge_list:
      logging.debug( 'Cleaning up branch "{0}"'.format( branch ) )
      git.remove_branch( branch )

  for merge in merge_list:
    branch_name = scm.mergeToBranch( merge )
    if project.commit_set.filter( branch=branch_name, done_at__isnull=True ).count() > 0:
      continue

    logging.info( '  Pulling MR/PR "{0}"'.format( merge ) )
    git.fetch_branch( scm.mergeToRef( merge ), branch_name )

  branch_map = git.ref_map()

  logging.debug( 'Branches: "{0}"'.format( branch_map ) )

  for branch in branch_map.keys():
    try:
      commit = Commit.objects.get( project=project, branch=branch, commit=branch_map[ branch ] )
      continue  # allready there, don't need to worry about it
    except Commit.DoesNotExist:
      pass

    logging.info( 'New Commit "{0}" on Branch "{1}" of Project "{2}"...'.format( branch_map[ branch ], branch, project.name ) )
    slack.post_message( 'New Commit "{0}" on Branch "{1}" of Project "{2}".'.format( branch_map[ branch ], branch, project.name ), slack.INFO )

    git.checkout( WORK_DIR, branch )

    commit = Commit()
    commit.project = project
    commit.branch = branch
    commit.commit = branch_map[ branch ]

    work_dir = glob.glob( '{0}/*'.format( WORK_DIR ) )[0]

    if not os.path.exists( os.path.join( work_dir, 'Makefile' ) ):
      logging.warning( 'No Makefile, ignorning commit.' )
      _failCommit( commit, 'No Makefile' )
      continue

    make = Makefile( work_dir )
    if not make.lint():
      logging.error( 'Makefile does not lint, ignorning commit.' )
      _failCommit( commit, 'Makefile does not lint' )
      continue

    version = make.version()
    if version is None:  # TODO: not ignore the version and just run the branch without tagging?
      logging.error( 'Unable to retrieve Version, ignorning commit.' )
      _failCommit( commit, 'Unable to retrieve Version' )
      continue

    commit.version = version

    if branch == project.release_branch:
      project.build_counter += 1
      project.full_clean()
      project.save()

    try:
      blueprint_list = make.testBluePrints()
    except MakeException as e:
      logging.error( 'Error Retreiving Test BluePrints from Makefile "{0}", ignorning commit.'.format( e ) )
      _failCommit( commit, 'Error Retreiving Test BluePrints from Makefile' )
      continue

    commit.lint_results = dict( zip( blueprint_list, [{}] * len( blueprint_list ) ) )
    commit.test_results = commit.lint_results
    commit.build_results = {}

    logging.info( 'Adding Package Build...' )

    for tmp in ( 'dpkg', 'rpm', 'respkg', 'resource' ):
      try:
        blueprint_list = make.packageBluePrints( tmp )
      except MakeException as e:
        logging.error( 'Error Retreiving Packaging BluePrints from Makefile "{0}", ignorning commit.'.format( e ) )
        _failCommit( commit, 'Error Retreiving Packaging BluePrints from Makefile' )
        continue

      if blueprint_list:
        commit.build_results[ tmp ] = dict( zip( blueprint_list, [{}] * len( blueprint_list ) ) )

    if branch == project.release_branch:
      logging.info( 'Adding Doc Build...' )
      try:
        blueprint_list = make.docBluePrints()
      except MakeException:
        blueprint_list = []

      commit.doc_results = dict( zip( blueprint_list, [{}] * len( blueprint_list ) ) )

    commit.build_results = commit.build_results

    if branch == project.release_branch:
      logging.info( 'Updating Builds info...' )
      try:
        auto_build_list = make.autoBuilds()
      except MakeException as e:
        logging.error( 'Error Retreiving Auto Builds from Makefile "{0}", ignorning commit.'.format( e ) )
        _failCommit( commit, 'Error Retreiving Auto Builds from Makefile' )
        continue

      try:
        manual_build_list = make.manualBuilds()
      except MakeException as e:
        logging.error( 'Error Retreiving Manual Builds from Makefile "{0}", ignorning commit.'.format( e ) )
        _failCommit( commit, 'Error Retreiving Manual Builds from Makefile' )
        continue

      logging.debug( 'Auto Builds "{0}".'.format( auto_build_list ) )
      logging.debug( 'Manual Builds "{0}".'.format( manual_build_list ) )

      existing_builds = [ i.name for i in project.build_set.all() ]
      for build_name in set( existing_builds ) - set( auto_build_list + manual_build_list ):
        build = project.build_set.get( name=build_name )
        try:
          build.delete()  # this could cause an error if there is a build curently operating, if so, punt to later
        except ProtectedError:
          logging.warning( 'Build "{0}" is no longer defined, however it is still in use, will try to delete again next time.'.format( build_name ) )
          pass  # can't delete now, will try on next update

      try:
        build = None
        for build in auto_build_list:
          msg = _makeBuild( build, project, False )
          if msg is not None:
            logging.error( msg )
            _failCommit( commit, msg )
            continue

        for build in manual_build_list:
          msg = _makeBuild( build, project, True )
          if msg is not None:
            logging.error( msg )
            _failCommit( commit, msg )
            continue

      except Exception as e:
        logging.exception( 'Error making build "{0}": "{1}", ignorning commit.'.format( build, e ) )
        _failCommit( commit, 'Error making build "{0}": "{1}", ignorning commit.'.format( build, e )  )
        continue

    logging.debug( 'Cleaning up work dir.' )
    shutil.rmtree( WORK_DIR )
    commit.full_clean()
    commit.save()

    commit.postInProcess()

  project.last_checked = datetime.now( timezone.utc )
  project.full_clean()
  project.save()

os.unlink( PID_FILE )
logging.info( 'Done!' )
logging.shutdown()
sys.exit( 0 )
